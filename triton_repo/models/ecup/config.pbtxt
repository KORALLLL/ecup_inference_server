name: "ecup"
backend: "python"
max_batch_size: 128

input [
  { name: "e5_input_ids"            data_type: TYPE_INT64 dims: [ -1 ] },
  { name: "e5_attention_mask"       data_type: TYPE_INT64 dims: [ -1 ] },
  { name: "bge_name_input_ids"      data_type: TYPE_INT64 dims: [ -1 ] },
  { name: "bge_name_attention_mask" data_type: TYPE_INT64 dims: [ -1 ] },
  { name: "bge_desc_input_ids"      data_type: TYPE_INT64 dims: [ -1 ] },
  { name: "bge_desc_attention_mask" data_type: TYPE_INT64 dims: [ -1 ] },
  { name: "x_numer"                 data_type: TYPE_FP32  dims: [ -1 ] }
]

output [
  { name: "probs" data_type: TYPE_FP32 dims: [ 1 ] }
]

instance_group [
  { kind: KIND_GPU count: 1 } 
]

dynamic_batching {
  preferred_batch_size: [ 8, 16, 32, 64, 128 ]
}
